<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>tests.test_migration API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>tests.test_migration</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import pytest
import subprocess
import common
from common import clients, volume_name, wait_for_volume_healthy  # NOQA
from common import get_random_client
from common import SIZE
from common import wait_for_volume_delete
from common import set_node_scheduling
from common import get_volume_endpoint, write_volume_dev_random_mb_data
from common import get_device_checksum
from common import wait_for_rebuild_start, wait_for_rebuild_complete
from common import Gi
from test_scheduling import get_host_replica
from common import client, core_api, storage_class, pvc_name # NOQA
from common import get_self_host_id, create_and_check_volume, create_backup
from common import create_storage_class, get_volume_engine
from common import create_rwx_volume_with_storageclass
from backupstore import set_random_backupstore # NOQA

REPLICA_COUNT = 2

@pytest.mark.coretest  # NOQA
@pytest.mark.migration # NOQA
def test_migration_confirm(clients, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test that a migratable RWX volume can be live migrated
    from one node to another.

    1. Creates a new RWX migratable volume.
    2. Attach to test node to write some test data on it.
    3. Detach from test node.
    4. Get set of nodes excluding the test node
    5. Attach volume to node 1 (initial node)
    6. Attach volume to node 2 (migration target)
    7. Wait for migration ready (engine running on node 2)
    8. Detach volume from node 1
    9. Observe volume migrated to node 2 (single active engine)
    10. Validate initially written test data
    &#34;&#34;&#34;
    migration_confirm_test(clients, volume_name)


def migration_confirm_test(clients, volume_name, backing_image=&#34;&#34;):  # NOQA
    client, volume, data = setup_migration_test(clients, volume_name, # NOQA
                                                backing_image)
    host1, host2 = get_hosts_for_migration_test(clients)

    attachment_id_1 = common.generate_attachment_ticket_id()
    volume.attach(attachmentID=attachment_id_1, hostId=host1,
                  attacherType=common.ATTACHER_TYPE_CSI_ATTACHER)
    volume = common.wait_for_volume_healthy(client, volume_name)

    attachment_id_2 = common.generate_attachment_ticket_id()
    volume.attach(attachmentID=attachment_id_2, hostId=host2,
                  attacherType=common.ATTACHER_TYPE_CSI_ATTACHER)
    volume = common.wait_for_volume_migration_ready(client, volume_name)

    volume.detach(attachmentID=attachment_id_1)
    volume = common.wait_for_volume_migration_node(client, volume_name, host2)

    volume.detach(attachmentID=attachment_id_2)
    volume = common.wait_for_volume_detached(client, volume_name)

    # verify test data
    check_detached_volume_data(client, volume_name, data)

    client.delete(volume)
    wait_for_volume_delete(client, volume_name)


@pytest.mark.coretest  # NOQA
@pytest.mark.migration # NOQA
def test_migration_rollback(clients, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test that a migratable RWX volume can be rolled back
    to initial node.

    1. Creates a new RWX migratable volume.
    2. Attach to test node to write some test data on it.
    3. Detach from test node.
    4. Get set of nodes excluding the test node
    5. Attach volume to node 1 (initial node)
    6. Attach volume to node 2 (migration target)
    7. Wait for migration ready (engine running on node 2)
    8. Detach volume from node 2
    9. Observe volume stayed on node 1 (single active engine)
    10. Validate initially written test data
    &#34;&#34;&#34;
    migration_rollback_test(clients, volume_name)


def migration_rollback_test(clients, volume_name, backing_image=&#34;&#34;):  # NOQA
    client, volume, data = setup_migration_test(clients, volume_name, # NOQA
                                                backing_image)
    host1, host2 = get_hosts_for_migration_test(clients)

    attachment_id_1 = common.generate_attachment_ticket_id()
    volume.attach(attachmentID=attachment_id_1, hostId=host1,
                  attacherType=common.ATTACHER_TYPE_CSI_ATTACHER)
    volume = common.wait_for_volume_healthy(client, volume_name)

    attachment_id_2 = common.generate_attachment_ticket_id()
    volume.attach(attachmentID=attachment_id_2, hostId=host2,
                  attacherType=common.ATTACHER_TYPE_CSI_ATTACHER)
    volume = common.wait_for_volume_migration_ready(client, volume_name)

    volume.detach(attachmentID=attachment_id_2)
    volume = common.wait_for_volume_migration_node(client, volume_name, host1)

    volume.detach(attachmentID=attachment_id_1)
    volume = common.wait_for_volume_detached(client, volume_name)

    # verify test data
    check_detached_volume_data(client, volume_name, data)

    client.delete(volume)
    wait_for_volume_delete(client, volume_name)


@pytest.mark.coretest  # NOQA
@pytest.mark.migration # NOQA
def test_migration_with_unscheduled_replica(clients, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test that a degraded migratable RWX volume that contain an unscheduled
    replica can be migrated.

    1. Disable the scheduling for one node.
    2. Create a new RWX migratable volume.
    3. Attach to test node to write some test data on it.
    4. Detach from test node.
    5. Get set of nodes excluding the test node.
    6. Attach volume to node 1 (initial node).
       The volume should be Degraded with an unscheduled replica.
    7. Attach volume to node 2 (migration target)
    8. Wait for migration ready (engine running on node 2).
       The newly created migration replica count should be the same as
       that of the current replicas.
       And there is one migration replica not scheduled, either.
    9. Detach volume from node 1.
    10. Observe volume migrated to node 2 (single active engine)
    11. Enable the scheduling for the node and wait for rebuilding complete.
    12. Validate initially written test data.
    &#34;&#34;&#34;
    # Step 1
    client = get_random_client(clients) # NOQA

    # local_node : write data / migrate target
    # hosts[0] : migrate initial node
    # hosts[1] : node to schedule on/off
    local_node = common.get_self_host_id()
    hosts = get_hosts_for_migration_test(clients)
    schedule_node = client.by_id_node(hosts[1])

    set_node_scheduling(client, schedule_node,
                        allowScheduling=False, retry=True)

    # Step 2,3,4
    volume = client.create_volume(name=volume_name, size=SIZE,
                                  numberOfReplicas=3,
                                  backingImage=&#34;&#34;,
                                  accessMode=&#34;rwx&#34;, migratable=True)
    volume = common.wait_for_volume_detached(client, volume_name)
    attachment_id = common.generate_attachment_ticket_id()
    volume.attach(attachmentID=attachment_id, hostId=local_node)
    volume = common.wait_for_volume_degraded(client, volume_name)

    data = common.write_volume_random_data(volume)
    common.check_volume_data(volume, data)

    # collect replicas name before detaching,
    # unscheduled replica would be deleted.
    old_replicas = []
    v = client.by_id_volume(volume_name)
    replicas = v.replicas
    for r in replicas:
        old_replicas.append(r.name)
    volume.detach(attachmentID=attachment_id)
    volume = common.wait_for_volume_detached(client, volume_name)

    # Step 6
    attachment_id_1 = common.generate_attachment_ticket_id()
    volume.attach(attachmentID=attachment_id_1, hostId=hosts[0],
                  attacherType=common.ATTACHER_TYPE_CSI_ATTACHER)
    volume = common.wait_for_volume_degraded(client, volume_name)

    # Step 7
    attachment_id_2 = common.generate_attachment_ticket_id()
    volume.attach(attachmentID=attachment_id_2, hostId=local_node,
                  attacherType=common.ATTACHER_TYPE_CSI_ATTACHER,)

    # Step 8
    volume = common.wait_for_volume_migration_ready(client, volume_name)
    volume = common.wait_for_volume_degraded(client, volume_name)

    new_replicas = []
    v = client.by_id_volume(volume_name)
    replicas = v.replicas
    for r in replicas:
        if r.name not in old_replicas:
            new_replicas.append(r.name)

    assert len(old_replicas) == len(new_replicas)

    # Step 9
    volume.detach(attachmentID=attachment_id_1)

    # Step 10
    volume = common.wait_for_volume_migration_node(client,
                                                   volume_name,
                                                   local_node)

    # Step 11
    set_node_scheduling(client, schedule_node,
                        allowScheduling=True, retry=True)
    volume = common.wait_for_volume_healthy(client, volume_name)

    # Step 12
    common.check_volume_data(volume, data)


@pytest.mark.coretest  # NOQA
@pytest.mark.migration # NOQA
def test_migration_with_failed_replica(clients, request, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test that a degraded migratable RWX volume that contain an failed replica
    can be migrated.

    1. Create a new RWX migratable volume.
    2. Attach to node 1 to write some test data on it.
    3. Remove the replica directory (/var/lib/longhorn/replicas) for one node.
       This makes one volume replica stay failed.
    4. Attach volume to node 2 (migration target).
    5. Wait for migration ready (engine running on node 2).
       The newly created migration replica count should be the same as
       that of the healthy current replicas.
    6. Detach volume from node 1.
    7. Observe volume migrated to node 2 (single active engine).
       And the old failed replica will be cleaned up.
    8. Validate initially written test data.
    &#34;&#34;&#34;
    def finalizer():
        exec_cmd = [&#34;mkdir&#34;, &#34;-p&#34;, &#34;/var/lib/longhorn/replicas&#34;]
        subprocess.check_output(exec_cmd)

    request.addfinalizer(finalizer)

    client, volume, data = setup_migration_test(clients, # NOQA
                                                volume_name,
                                                replica_cnt=3)

    current_node = common.get_self_host_id()
    hosts = get_hosts_for_migration_test(clients)
    migrate_target = hosts[0]
    attachment_id_1 = common.generate_attachment_ticket_id()
    volume.attach(attachmentID=attachment_id_1, hostId=current_node,
                  attacherType=common.ATTACHER_TYPE_CSI_ATTACHER)
    volume = common.wait_for_volume_healthy(client, volume_name)

    old_replicas = []
    for replica in volume.replicas:
        old_replicas.append(replica.name)

    exec_cmd = [&#34;rm&#34;, &#34;-rf&#34;,  &#34;/var/lib/longhorn/replicas&#34;]
    subprocess.check_output(exec_cmd)
    volume = common.wait_for_volume_degraded(client, volume_name)

    attachment_id_2 = common.generate_attachment_ticket_id()
    volume.attach(attachmentID=attachment_id_2, hostId=migrate_target,
                  attacherType=common.ATTACHER_TYPE_CSI_ATTACHER)
    volume = common.wait_for_volume_migration_ready(client, volume_name)

    new_replicas = 0
    for replica in volume.replicas:
        if replica.hostId == current_node:
            assert replica.running is False
        else:
            assert replica.running is True
            if replica.name not in old_replicas:
                new_replicas = new_replicas + 1
    assert new_replicas == 2

    volume.detach(attachmentID=attachment_id_1)

    volume = common.wait_for_volume_migration_node(client,
                                                   volume_name,
                                                   migrate_target)

    volume.updateReplicaCount(replicaCount=2)
    volume = common.wait_for_volume_healthy(client, volume_name)

    volume.detach(attachmentID=attachment_id_2)
    volume = common.wait_for_volume_detached(client, volume_name)
    volume = client.by_id_volume(volume_name)
    check_detached_volume_data(client, volume_name, data)


@pytest.mark.coretest  # NOQA
@pytest.mark.migration # NOQA
def test_migration_with_rebuilding_replica(clients, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test that a degraded migratable RWX volume that contain a rebuilding
    replica can be migrated.

    1. Create a new RWX migratable volume.
    2. Attach to node 1, then write a large amount if data on it so that
       the following rebuilding will take a while.
    3. Remove one healthy replica to trigger rebuilding.
    4. Immediately attach volume to node 2 (migration target) once
       the rebuilding starts.
       There should be no replica created before rebuilding complete.
    5. Wait for rebuilding complete then migration ready
       (engine running on node 2).
       The newly created migration replica count should be the same as
       that of the current replicas.
    6. Detach volume from node 1.
    7. Observe volume migrated to node 2 (single active engine).
    8. Validate initially written test data.
    &#34;&#34;&#34;
    # Step 1
    client = get_random_client(clients) # NOQA
    current_host = common.get_self_host_id()
    host1, host2 = get_hosts_for_migration_test(clients)

    # Step 2
    volume = client.create_volume(name=volume_name, size=str(2 * Gi),
                                  numberOfReplicas=3,
                                  backingImage=&#34;&#34;,
                                  accessMode=&#34;rwx&#34;, migratable=True)
    volume = common.wait_for_volume_detached(client, volume_name)
    attachment_id_1 = common.generate_attachment_ticket_id()
    volume.attach(attachmentID=attachment_id_1, hostId=current_host,
                  attacherType=common.ATTACHER_TYPE_CSI_ATTACHER)
    volume = common.wait_for_volume_healthy(client, volume_name)
    old_replicas = volume.replicas

    volume_endpoint = get_volume_endpoint(volume)
    write_volume_dev_random_mb_data(volume_endpoint,
                                    1, 1500)
    data = get_device_checksum(volume_endpoint)

    # Step 3
    host_replica = get_host_replica(volume, host_id=current_host)
    volume.replicaRemove(name=host_replica.name)

    # Step 4
    wait_for_rebuild_start(client, volume_name)

    attachment_id_2 = common.generate_attachment_ticket_id()
    volume.attach(attachmentID=attachment_id_2, hostId=host1,
                  attacherType=common.ATTACHER_TYPE_CSI_ATTACHER)
    volume = client.by_id_volume(volume_name)
    assert len(volume.replicas) == len(old_replicas)

    wait_for_rebuild_complete(client, volume_name)

    # Step 5
    volume = common.wait_for_volume_migration_ready(client, volume_name)
    new_replicas = volume.replicas
    assert len(old_replicas) == (len(new_replicas) - len(old_replicas))

    # Step 6
    volume.detach(attachmentID=attachment_id_1)

    # Step 7
    volume = common.wait_for_volume_migration_node(client,
                                                   volume_name,
                                                   host1)
    volume = common.wait_for_volume_healthy(client, volume_name)

    replicas = volume.replicas
    assert len(replicas) == len(old_replicas)

    # Step 8
    volume.detach(attachmentID=attachment_id_2)

    volume = common.wait_for_volume_detached(client, volume_name)
    attachment_id = common.generate_attachment_ticket_id()
    volume.attach(attachmentID=attachment_id, hostId=current_host)
    volume = common.wait_for_volume_healthy(client, volume_name)
    volume_endpoint = get_volume_endpoint(volume)
    assert data == get_device_checksum(volume_endpoint)


@pytest.mark.coretest  # NOQA
@pytest.mark.migration # NOQA
def test_migration_with_restore_volume(core_api, # NOQA
                                       client, # NOQA
                                       clients, # NOQA
                                       volume_name, # NOQA
                                       storage_class, # NOQA
                                       pvc_name, # NOQA
                                       set_random_backupstore):  # NOQA
    &#34;&#34;&#34;
    Test that a restored volume can be migrated.
    1. Prepare one backup.
    2. Create a StorageClass with `migratable` being enabled and
       `fromBackup` pointing to the above backup.
    3. Create a new RWX migratable volume using the StorageClass.
    4. Attach to node 1, then write some data.
    5. Attach the volume to node 2 (migration target).
    6. Wait for the migration ready. Verify that field
       `volume.controllers[0].requestedBackupRestore` is empty.
    7. Confirm the migration then validate the data.
    &#34;&#34;&#34;
    # Step 1
    lht_host_id = get_self_host_id()
    volume = create_and_check_volume(client,
                                     volume_name,
                                     REPLICA_COUNT,
                                     SIZE)

    attachment_id = common.generate_attachment_ticket_id()
    volume.attach(attachmentID=attachment_id, hostId=lht_host_id)
    volume = common.wait_for_volume_healthy(client, volume_name)
    common.write_volume_random_data(volume)
    bv, b, _, _ = create_backup(client, volume_name)

    # Step 2
    storage_class[&#39;metadata&#39;][&#39;name&#39;] = &#34;longhorn-from-backup&#34;
    storage_class[&#39;parameters&#39;][&#39;fromBackup&#39;] = b.url
    storage_class[&#39;parameters&#39;][&#39;migratable&#39;] = &#34;true&#34;

    create_storage_class(storage_class)

    # Stpe 3
    backup_volume_name = \
        create_rwx_volume_with_storageclass(client,
                                            core_api,
                                            storage_class)

    restored_volume = client.by_id_volume(backup_volume_name)
    volume_engine = get_volume_engine(restored_volume)
    assert volume_engine.requestedBackupRestore == &#34;&#34;

    # Step 4
    lht_host_id = common.get_self_host_id()
    host1, host2 = get_hosts_for_migration_test(clients)

    volume = client.by_id_volume(backup_volume_name)
    attachment_id = common.generate_attachment_ticket_id()
    volume.attach(attachmentID=attachment_id, hostId=lht_host_id)
    volume = common.wait_for_volume_healthy(client, backup_volume_name)

    data = common.write_volume_random_data(volume)
    volume.detach(attachmentID=attachment_id)
    volume = common.wait_for_volume_detached(client, backup_volume_name)

    # Step 5, 6
    attachment_id_1 = common.generate_attachment_ticket_id()
    volume.attach(attachmentID=attachment_id_1, hostId=host1,
                  attacherType=common.ATTACHER_TYPE_CSI_ATTACHER)
    volume = common.wait_for_volume_healthy(client, backup_volume_name)

    attachment_id_2 = common.generate_attachment_ticket_id()
    volume.attach(attachmentID=attachment_id_2, hostId=host2,
                  attacherType=common.ATTACHER_TYPE_CSI_ATTACHER)
    volume = common.wait_for_volume_migration_ready(client, backup_volume_name)

    volume.detach(attachmentID=attachment_id_1)
    volume = common.wait_for_volume_migration_node(client,
                                                   backup_volume_name,
                                                   host2)

    volume.detach(attachmentID=attachment_id_2)
    volume = common.wait_for_volume_detached(client, backup_volume_name)

    # verify test data
    check_detached_volume_data(client, backup_volume_name, data)


def get_hosts_for_migration_test(clients): # NOQA
    &#34;&#34;&#34;
    Filters out the current node from the returned hosts list

    We use the current node for device writing before the test
    and verification of the data after the test
    &#34;&#34;&#34;
    hosts = []
    current_host = common.get_self_host_id()
    for host in list(clients):
        if host != current_host:
            hosts.append(host)
    return hosts[0], hosts[1]


def check_detached_volume_data(client, volume_name, data): # NOQA
    &#34;&#34;&#34;
    Attaches the volume to the current node
    then compares the volumes data
    against the passed data.
    &#34;&#34;&#34;
    volume = client.by_id_volume(volume_name)
    attachment_id = common.generate_attachment_ticket_id()
    volume.attach(attachmentID=attachment_id, hostId=common.get_self_host_id())
    volume = common.wait_for_volume_healthy(client, volume_name)
    common.check_volume_data(volume, data)

    volume.detach(attachmentID=attachment_id)
    volume = common.wait_for_volume_detached(client, volume_name)


def setup_migration_test(clients, volume_name, backing_image=&#34;&#34;, replica_cnt=REPLICA_COUNT): # NOQA
    &#34;&#34;&#34;
    Creates a new migratable volume then attaches it to the
    current node to write some test data on it.
    &#34;&#34;&#34;
    client = get_random_client(clients) # NOQA
    volume = client.create_volume(name=volume_name, size=SIZE,
                                  numberOfReplicas=replica_cnt,
                                  backingImage=backing_image,
                                  accessMode=&#34;rwx&#34;, migratable=True)
    volume = common.wait_for_volume_detached(client, volume_name)
    attachment_id = common.generate_attachment_ticket_id()
    volume.attach(attachmentID=attachment_id, hostId=common.get_self_host_id())
    volume = common.wait_for_volume_healthy(client, volume_name)

    # write test data
    data = common.write_volume_random_data(volume)
    common.check_volume_data(volume, data)

    volume.detach(attachmentID=attachment_id)
    volume = common.wait_for_volume_detached(client, volume_name)
    return client, volume, data</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="tests.test_migration.check_detached_volume_data"><code class="name flex">
<span>def <span class="ident">check_detached_volume_data</span></span>(<span>client, volume_name, data)</span>
</code></dt>
<dd>
<div class="desc"><p>Attaches the volume to the current node
then compares the volumes data
against the passed data.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def check_detached_volume_data(client, volume_name, data): # NOQA
    &#34;&#34;&#34;
    Attaches the volume to the current node
    then compares the volumes data
    against the passed data.
    &#34;&#34;&#34;
    volume = client.by_id_volume(volume_name)
    attachment_id = common.generate_attachment_ticket_id()
    volume.attach(attachmentID=attachment_id, hostId=common.get_self_host_id())
    volume = common.wait_for_volume_healthy(client, volume_name)
    common.check_volume_data(volume, data)

    volume.detach(attachmentID=attachment_id)
    volume = common.wait_for_volume_detached(client, volume_name)</code></pre>
</details>
</dd>
<dt id="tests.test_migration.get_hosts_for_migration_test"><code class="name flex">
<span>def <span class="ident">get_hosts_for_migration_test</span></span>(<span>clients)</span>
</code></dt>
<dd>
<div class="desc"><p>Filters out the current node from the returned hosts list</p>
<p>We use the current node for device writing before the test
and verification of the data after the test</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_hosts_for_migration_test(clients): # NOQA
    &#34;&#34;&#34;
    Filters out the current node from the returned hosts list

    We use the current node for device writing before the test
    and verification of the data after the test
    &#34;&#34;&#34;
    hosts = []
    current_host = common.get_self_host_id()
    for host in list(clients):
        if host != current_host:
            hosts.append(host)
    return hosts[0], hosts[1]</code></pre>
</details>
</dd>
<dt id="tests.test_migration.migration_confirm_test"><code class="name flex">
<span>def <span class="ident">migration_confirm_test</span></span>(<span>clients, volume_name, backing_image='')</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def migration_confirm_test(clients, volume_name, backing_image=&#34;&#34;):  # NOQA
    client, volume, data = setup_migration_test(clients, volume_name, # NOQA
                                                backing_image)
    host1, host2 = get_hosts_for_migration_test(clients)

    attachment_id_1 = common.generate_attachment_ticket_id()
    volume.attach(attachmentID=attachment_id_1, hostId=host1,
                  attacherType=common.ATTACHER_TYPE_CSI_ATTACHER)
    volume = common.wait_for_volume_healthy(client, volume_name)

    attachment_id_2 = common.generate_attachment_ticket_id()
    volume.attach(attachmentID=attachment_id_2, hostId=host2,
                  attacherType=common.ATTACHER_TYPE_CSI_ATTACHER)
    volume = common.wait_for_volume_migration_ready(client, volume_name)

    volume.detach(attachmentID=attachment_id_1)
    volume = common.wait_for_volume_migration_node(client, volume_name, host2)

    volume.detach(attachmentID=attachment_id_2)
    volume = common.wait_for_volume_detached(client, volume_name)

    # verify test data
    check_detached_volume_data(client, volume_name, data)

    client.delete(volume)
    wait_for_volume_delete(client, volume_name)</code></pre>
</details>
</dd>
<dt id="tests.test_migration.migration_rollback_test"><code class="name flex">
<span>def <span class="ident">migration_rollback_test</span></span>(<span>clients, volume_name, backing_image='')</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def migration_rollback_test(clients, volume_name, backing_image=&#34;&#34;):  # NOQA
    client, volume, data = setup_migration_test(clients, volume_name, # NOQA
                                                backing_image)
    host1, host2 = get_hosts_for_migration_test(clients)

    attachment_id_1 = common.generate_attachment_ticket_id()
    volume.attach(attachmentID=attachment_id_1, hostId=host1,
                  attacherType=common.ATTACHER_TYPE_CSI_ATTACHER)
    volume = common.wait_for_volume_healthy(client, volume_name)

    attachment_id_2 = common.generate_attachment_ticket_id()
    volume.attach(attachmentID=attachment_id_2, hostId=host2,
                  attacherType=common.ATTACHER_TYPE_CSI_ATTACHER)
    volume = common.wait_for_volume_migration_ready(client, volume_name)

    volume.detach(attachmentID=attachment_id_2)
    volume = common.wait_for_volume_migration_node(client, volume_name, host1)

    volume.detach(attachmentID=attachment_id_1)
    volume = common.wait_for_volume_detached(client, volume_name)

    # verify test data
    check_detached_volume_data(client, volume_name, data)

    client.delete(volume)
    wait_for_volume_delete(client, volume_name)</code></pre>
</details>
</dd>
<dt id="tests.test_migration.setup_migration_test"><code class="name flex">
<span>def <span class="ident">setup_migration_test</span></span>(<span>clients, volume_name, backing_image='', replica_cnt=2)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a new migratable volume then attaches it to the
current node to write some test data on it.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def setup_migration_test(clients, volume_name, backing_image=&#34;&#34;, replica_cnt=REPLICA_COUNT): # NOQA
    &#34;&#34;&#34;
    Creates a new migratable volume then attaches it to the
    current node to write some test data on it.
    &#34;&#34;&#34;
    client = get_random_client(clients) # NOQA
    volume = client.create_volume(name=volume_name, size=SIZE,
                                  numberOfReplicas=replica_cnt,
                                  backingImage=backing_image,
                                  accessMode=&#34;rwx&#34;, migratable=True)
    volume = common.wait_for_volume_detached(client, volume_name)
    attachment_id = common.generate_attachment_ticket_id()
    volume.attach(attachmentID=attachment_id, hostId=common.get_self_host_id())
    volume = common.wait_for_volume_healthy(client, volume_name)

    # write test data
    data = common.write_volume_random_data(volume)
    common.check_volume_data(volume, data)

    volume.detach(attachmentID=attachment_id)
    volume = common.wait_for_volume_detached(client, volume_name)
    return client, volume, data</code></pre>
</details>
</dd>
<dt id="tests.test_migration.test_migration_confirm"><code class="name flex">
<span>def <span class="ident">test_migration_confirm</span></span>(<span>clients, volume_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Test that a migratable RWX volume can be live migrated
from one node to another.</p>
<ol>
<li>Creates a new RWX migratable volume.</li>
<li>Attach to test node to write some test data on it.</li>
<li>Detach from test node.</li>
<li>Get set of nodes excluding the test node</li>
<li>Attach volume to node 1 (initial node)</li>
<li>Attach volume to node 2 (migration target)</li>
<li>Wait for migration ready (engine running on node 2)</li>
<li>Detach volume from node 1</li>
<li>Observe volume migrated to node 2 (single active engine)</li>
<li>Validate initially written test data</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.coretest  # NOQA
@pytest.mark.migration # NOQA
def test_migration_confirm(clients, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test that a migratable RWX volume can be live migrated
    from one node to another.

    1. Creates a new RWX migratable volume.
    2. Attach to test node to write some test data on it.
    3. Detach from test node.
    4. Get set of nodes excluding the test node
    5. Attach volume to node 1 (initial node)
    6. Attach volume to node 2 (migration target)
    7. Wait for migration ready (engine running on node 2)
    8. Detach volume from node 1
    9. Observe volume migrated to node 2 (single active engine)
    10. Validate initially written test data
    &#34;&#34;&#34;
    migration_confirm_test(clients, volume_name)</code></pre>
</details>
</dd>
<dt id="tests.test_migration.test_migration_rollback"><code class="name flex">
<span>def <span class="ident">test_migration_rollback</span></span>(<span>clients, volume_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Test that a migratable RWX volume can be rolled back
to initial node.</p>
<ol>
<li>Creates a new RWX migratable volume.</li>
<li>Attach to test node to write some test data on it.</li>
<li>Detach from test node.</li>
<li>Get set of nodes excluding the test node</li>
<li>Attach volume to node 1 (initial node)</li>
<li>Attach volume to node 2 (migration target)</li>
<li>Wait for migration ready (engine running on node 2)</li>
<li>Detach volume from node 2</li>
<li>Observe volume stayed on node 1 (single active engine)</li>
<li>Validate initially written test data</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.coretest  # NOQA
@pytest.mark.migration # NOQA
def test_migration_rollback(clients, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test that a migratable RWX volume can be rolled back
    to initial node.

    1. Creates a new RWX migratable volume.
    2. Attach to test node to write some test data on it.
    3. Detach from test node.
    4. Get set of nodes excluding the test node
    5. Attach volume to node 1 (initial node)
    6. Attach volume to node 2 (migration target)
    7. Wait for migration ready (engine running on node 2)
    8. Detach volume from node 2
    9. Observe volume stayed on node 1 (single active engine)
    10. Validate initially written test data
    &#34;&#34;&#34;
    migration_rollback_test(clients, volume_name)</code></pre>
</details>
</dd>
<dt id="tests.test_migration.test_migration_with_failed_replica"><code class="name flex">
<span>def <span class="ident">test_migration_with_failed_replica</span></span>(<span>clients, request, volume_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Test that a degraded migratable RWX volume that contain an failed replica
can be migrated.</p>
<ol>
<li>Create a new RWX migratable volume.</li>
<li>Attach to node 1 to write some test data on it.</li>
<li>Remove the replica directory (/var/lib/longhorn/replicas) for one node.
This makes one volume replica stay failed.</li>
<li>Attach volume to node 2 (migration target).</li>
<li>Wait for migration ready (engine running on node 2).
The newly created migration replica count should be the same as
that of the healthy current replicas.</li>
<li>Detach volume from node 1.</li>
<li>Observe volume migrated to node 2 (single active engine).
And the old failed replica will be cleaned up.</li>
<li>Validate initially written test data.</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.coretest  # NOQA
@pytest.mark.migration # NOQA
def test_migration_with_failed_replica(clients, request, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test that a degraded migratable RWX volume that contain an failed replica
    can be migrated.

    1. Create a new RWX migratable volume.
    2. Attach to node 1 to write some test data on it.
    3. Remove the replica directory (/var/lib/longhorn/replicas) for one node.
       This makes one volume replica stay failed.
    4. Attach volume to node 2 (migration target).
    5. Wait for migration ready (engine running on node 2).
       The newly created migration replica count should be the same as
       that of the healthy current replicas.
    6. Detach volume from node 1.
    7. Observe volume migrated to node 2 (single active engine).
       And the old failed replica will be cleaned up.
    8. Validate initially written test data.
    &#34;&#34;&#34;
    def finalizer():
        exec_cmd = [&#34;mkdir&#34;, &#34;-p&#34;, &#34;/var/lib/longhorn/replicas&#34;]
        subprocess.check_output(exec_cmd)

    request.addfinalizer(finalizer)

    client, volume, data = setup_migration_test(clients, # NOQA
                                                volume_name,
                                                replica_cnt=3)

    current_node = common.get_self_host_id()
    hosts = get_hosts_for_migration_test(clients)
    migrate_target = hosts[0]
    attachment_id_1 = common.generate_attachment_ticket_id()
    volume.attach(attachmentID=attachment_id_1, hostId=current_node,
                  attacherType=common.ATTACHER_TYPE_CSI_ATTACHER)
    volume = common.wait_for_volume_healthy(client, volume_name)

    old_replicas = []
    for replica in volume.replicas:
        old_replicas.append(replica.name)

    exec_cmd = [&#34;rm&#34;, &#34;-rf&#34;,  &#34;/var/lib/longhorn/replicas&#34;]
    subprocess.check_output(exec_cmd)
    volume = common.wait_for_volume_degraded(client, volume_name)

    attachment_id_2 = common.generate_attachment_ticket_id()
    volume.attach(attachmentID=attachment_id_2, hostId=migrate_target,
                  attacherType=common.ATTACHER_TYPE_CSI_ATTACHER)
    volume = common.wait_for_volume_migration_ready(client, volume_name)

    new_replicas = 0
    for replica in volume.replicas:
        if replica.hostId == current_node:
            assert replica.running is False
        else:
            assert replica.running is True
            if replica.name not in old_replicas:
                new_replicas = new_replicas + 1
    assert new_replicas == 2

    volume.detach(attachmentID=attachment_id_1)

    volume = common.wait_for_volume_migration_node(client,
                                                   volume_name,
                                                   migrate_target)

    volume.updateReplicaCount(replicaCount=2)
    volume = common.wait_for_volume_healthy(client, volume_name)

    volume.detach(attachmentID=attachment_id_2)
    volume = common.wait_for_volume_detached(client, volume_name)
    volume = client.by_id_volume(volume_name)
    check_detached_volume_data(client, volume_name, data)</code></pre>
</details>
</dd>
<dt id="tests.test_migration.test_migration_with_rebuilding_replica"><code class="name flex">
<span>def <span class="ident">test_migration_with_rebuilding_replica</span></span>(<span>clients, volume_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Test that a degraded migratable RWX volume that contain a rebuilding
replica can be migrated.</p>
<ol>
<li>Create a new RWX migratable volume.</li>
<li>Attach to node 1, then write a large amount if data on it so that
the following rebuilding will take a while.</li>
<li>Remove one healthy replica to trigger rebuilding.</li>
<li>Immediately attach volume to node 2 (migration target) once
the rebuilding starts.
There should be no replica created before rebuilding complete.</li>
<li>Wait for rebuilding complete then migration ready
(engine running on node 2).
The newly created migration replica count should be the same as
that of the current replicas.</li>
<li>Detach volume from node 1.</li>
<li>Observe volume migrated to node 2 (single active engine).</li>
<li>Validate initially written test data.</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.coretest  # NOQA
@pytest.mark.migration # NOQA
def test_migration_with_rebuilding_replica(clients, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test that a degraded migratable RWX volume that contain a rebuilding
    replica can be migrated.

    1. Create a new RWX migratable volume.
    2. Attach to node 1, then write a large amount if data on it so that
       the following rebuilding will take a while.
    3. Remove one healthy replica to trigger rebuilding.
    4. Immediately attach volume to node 2 (migration target) once
       the rebuilding starts.
       There should be no replica created before rebuilding complete.
    5. Wait for rebuilding complete then migration ready
       (engine running on node 2).
       The newly created migration replica count should be the same as
       that of the current replicas.
    6. Detach volume from node 1.
    7. Observe volume migrated to node 2 (single active engine).
    8. Validate initially written test data.
    &#34;&#34;&#34;
    # Step 1
    client = get_random_client(clients) # NOQA
    current_host = common.get_self_host_id()
    host1, host2 = get_hosts_for_migration_test(clients)

    # Step 2
    volume = client.create_volume(name=volume_name, size=str(2 * Gi),
                                  numberOfReplicas=3,
                                  backingImage=&#34;&#34;,
                                  accessMode=&#34;rwx&#34;, migratable=True)
    volume = common.wait_for_volume_detached(client, volume_name)
    attachment_id_1 = common.generate_attachment_ticket_id()
    volume.attach(attachmentID=attachment_id_1, hostId=current_host,
                  attacherType=common.ATTACHER_TYPE_CSI_ATTACHER)
    volume = common.wait_for_volume_healthy(client, volume_name)
    old_replicas = volume.replicas

    volume_endpoint = get_volume_endpoint(volume)
    write_volume_dev_random_mb_data(volume_endpoint,
                                    1, 1500)
    data = get_device_checksum(volume_endpoint)

    # Step 3
    host_replica = get_host_replica(volume, host_id=current_host)
    volume.replicaRemove(name=host_replica.name)

    # Step 4
    wait_for_rebuild_start(client, volume_name)

    attachment_id_2 = common.generate_attachment_ticket_id()
    volume.attach(attachmentID=attachment_id_2, hostId=host1,
                  attacherType=common.ATTACHER_TYPE_CSI_ATTACHER)
    volume = client.by_id_volume(volume_name)
    assert len(volume.replicas) == len(old_replicas)

    wait_for_rebuild_complete(client, volume_name)

    # Step 5
    volume = common.wait_for_volume_migration_ready(client, volume_name)
    new_replicas = volume.replicas
    assert len(old_replicas) == (len(new_replicas) - len(old_replicas))

    # Step 6
    volume.detach(attachmentID=attachment_id_1)

    # Step 7
    volume = common.wait_for_volume_migration_node(client,
                                                   volume_name,
                                                   host1)
    volume = common.wait_for_volume_healthy(client, volume_name)

    replicas = volume.replicas
    assert len(replicas) == len(old_replicas)

    # Step 8
    volume.detach(attachmentID=attachment_id_2)

    volume = common.wait_for_volume_detached(client, volume_name)
    attachment_id = common.generate_attachment_ticket_id()
    volume.attach(attachmentID=attachment_id, hostId=current_host)
    volume = common.wait_for_volume_healthy(client, volume_name)
    volume_endpoint = get_volume_endpoint(volume)
    assert data == get_device_checksum(volume_endpoint)</code></pre>
</details>
</dd>
<dt id="tests.test_migration.test_migration_with_restore_volume"><code class="name flex">
<span>def <span class="ident">test_migration_with_restore_volume</span></span>(<span>core_api, client, clients, volume_name, storage_class, pvc_name, set_random_backupstore)</span>
</code></dt>
<dd>
<div class="desc"><p>Test that a restored volume can be migrated.
1. Prepare one backup.
2. Create a StorageClass with <code>migratable</code> being enabled and
<code>fromBackup</code> pointing to the above backup.
3. Create a new RWX migratable volume using the StorageClass.
4. Attach to node 1, then write some data.
5. Attach the volume to node 2 (migration target).
6. Wait for the migration ready. Verify that field
<code>volume.controllers[0].requestedBackupRestore</code> is empty.
7. Confirm the migration then validate the data.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.coretest  # NOQA
@pytest.mark.migration # NOQA
def test_migration_with_restore_volume(core_api, # NOQA
                                       client, # NOQA
                                       clients, # NOQA
                                       volume_name, # NOQA
                                       storage_class, # NOQA
                                       pvc_name, # NOQA
                                       set_random_backupstore):  # NOQA
    &#34;&#34;&#34;
    Test that a restored volume can be migrated.
    1. Prepare one backup.
    2. Create a StorageClass with `migratable` being enabled and
       `fromBackup` pointing to the above backup.
    3. Create a new RWX migratable volume using the StorageClass.
    4. Attach to node 1, then write some data.
    5. Attach the volume to node 2 (migration target).
    6. Wait for the migration ready. Verify that field
       `volume.controllers[0].requestedBackupRestore` is empty.
    7. Confirm the migration then validate the data.
    &#34;&#34;&#34;
    # Step 1
    lht_host_id = get_self_host_id()
    volume = create_and_check_volume(client,
                                     volume_name,
                                     REPLICA_COUNT,
                                     SIZE)

    attachment_id = common.generate_attachment_ticket_id()
    volume.attach(attachmentID=attachment_id, hostId=lht_host_id)
    volume = common.wait_for_volume_healthy(client, volume_name)
    common.write_volume_random_data(volume)
    bv, b, _, _ = create_backup(client, volume_name)

    # Step 2
    storage_class[&#39;metadata&#39;][&#39;name&#39;] = &#34;longhorn-from-backup&#34;
    storage_class[&#39;parameters&#39;][&#39;fromBackup&#39;] = b.url
    storage_class[&#39;parameters&#39;][&#39;migratable&#39;] = &#34;true&#34;

    create_storage_class(storage_class)

    # Stpe 3
    backup_volume_name = \
        create_rwx_volume_with_storageclass(client,
                                            core_api,
                                            storage_class)

    restored_volume = client.by_id_volume(backup_volume_name)
    volume_engine = get_volume_engine(restored_volume)
    assert volume_engine.requestedBackupRestore == &#34;&#34;

    # Step 4
    lht_host_id = common.get_self_host_id()
    host1, host2 = get_hosts_for_migration_test(clients)

    volume = client.by_id_volume(backup_volume_name)
    attachment_id = common.generate_attachment_ticket_id()
    volume.attach(attachmentID=attachment_id, hostId=lht_host_id)
    volume = common.wait_for_volume_healthy(client, backup_volume_name)

    data = common.write_volume_random_data(volume)
    volume.detach(attachmentID=attachment_id)
    volume = common.wait_for_volume_detached(client, backup_volume_name)

    # Step 5, 6
    attachment_id_1 = common.generate_attachment_ticket_id()
    volume.attach(attachmentID=attachment_id_1, hostId=host1,
                  attacherType=common.ATTACHER_TYPE_CSI_ATTACHER)
    volume = common.wait_for_volume_healthy(client, backup_volume_name)

    attachment_id_2 = common.generate_attachment_ticket_id()
    volume.attach(attachmentID=attachment_id_2, hostId=host2,
                  attacherType=common.ATTACHER_TYPE_CSI_ATTACHER)
    volume = common.wait_for_volume_migration_ready(client, backup_volume_name)

    volume.detach(attachmentID=attachment_id_1)
    volume = common.wait_for_volume_migration_node(client,
                                                   backup_volume_name,
                                                   host2)

    volume.detach(attachmentID=attachment_id_2)
    volume = common.wait_for_volume_detached(client, backup_volume_name)

    # verify test data
    check_detached_volume_data(client, backup_volume_name, data)</code></pre>
</details>
</dd>
<dt id="tests.test_migration.test_migration_with_unscheduled_replica"><code class="name flex">
<span>def <span class="ident">test_migration_with_unscheduled_replica</span></span>(<span>clients, volume_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Test that a degraded migratable RWX volume that contain an unscheduled
replica can be migrated.</p>
<ol>
<li>Disable the scheduling for one node.</li>
<li>Create a new RWX migratable volume.</li>
<li>Attach to test node to write some test data on it.</li>
<li>Detach from test node.</li>
<li>Get set of nodes excluding the test node.</li>
<li>Attach volume to node 1 (initial node).
The volume should be Degraded with an unscheduled replica.</li>
<li>Attach volume to node 2 (migration target)</li>
<li>Wait for migration ready (engine running on node 2).
The newly created migration replica count should be the same as
that of the current replicas.
And there is one migration replica not scheduled, either.</li>
<li>Detach volume from node 1.</li>
<li>Observe volume migrated to node 2 (single active engine)</li>
<li>Enable the scheduling for the node and wait for rebuilding complete.</li>
<li>Validate initially written test data.</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.coretest  # NOQA
@pytest.mark.migration # NOQA
def test_migration_with_unscheduled_replica(clients, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test that a degraded migratable RWX volume that contain an unscheduled
    replica can be migrated.

    1. Disable the scheduling for one node.
    2. Create a new RWX migratable volume.
    3. Attach to test node to write some test data on it.
    4. Detach from test node.
    5. Get set of nodes excluding the test node.
    6. Attach volume to node 1 (initial node).
       The volume should be Degraded with an unscheduled replica.
    7. Attach volume to node 2 (migration target)
    8. Wait for migration ready (engine running on node 2).
       The newly created migration replica count should be the same as
       that of the current replicas.
       And there is one migration replica not scheduled, either.
    9. Detach volume from node 1.
    10. Observe volume migrated to node 2 (single active engine)
    11. Enable the scheduling for the node and wait for rebuilding complete.
    12. Validate initially written test data.
    &#34;&#34;&#34;
    # Step 1
    client = get_random_client(clients) # NOQA

    # local_node : write data / migrate target
    # hosts[0] : migrate initial node
    # hosts[1] : node to schedule on/off
    local_node = common.get_self_host_id()
    hosts = get_hosts_for_migration_test(clients)
    schedule_node = client.by_id_node(hosts[1])

    set_node_scheduling(client, schedule_node,
                        allowScheduling=False, retry=True)

    # Step 2,3,4
    volume = client.create_volume(name=volume_name, size=SIZE,
                                  numberOfReplicas=3,
                                  backingImage=&#34;&#34;,
                                  accessMode=&#34;rwx&#34;, migratable=True)
    volume = common.wait_for_volume_detached(client, volume_name)
    attachment_id = common.generate_attachment_ticket_id()
    volume.attach(attachmentID=attachment_id, hostId=local_node)
    volume = common.wait_for_volume_degraded(client, volume_name)

    data = common.write_volume_random_data(volume)
    common.check_volume_data(volume, data)

    # collect replicas name before detaching,
    # unscheduled replica would be deleted.
    old_replicas = []
    v = client.by_id_volume(volume_name)
    replicas = v.replicas
    for r in replicas:
        old_replicas.append(r.name)
    volume.detach(attachmentID=attachment_id)
    volume = common.wait_for_volume_detached(client, volume_name)

    # Step 6
    attachment_id_1 = common.generate_attachment_ticket_id()
    volume.attach(attachmentID=attachment_id_1, hostId=hosts[0],
                  attacherType=common.ATTACHER_TYPE_CSI_ATTACHER)
    volume = common.wait_for_volume_degraded(client, volume_name)

    # Step 7
    attachment_id_2 = common.generate_attachment_ticket_id()
    volume.attach(attachmentID=attachment_id_2, hostId=local_node,
                  attacherType=common.ATTACHER_TYPE_CSI_ATTACHER,)

    # Step 8
    volume = common.wait_for_volume_migration_ready(client, volume_name)
    volume = common.wait_for_volume_degraded(client, volume_name)

    new_replicas = []
    v = client.by_id_volume(volume_name)
    replicas = v.replicas
    for r in replicas:
        if r.name not in old_replicas:
            new_replicas.append(r.name)

    assert len(old_replicas) == len(new_replicas)

    # Step 9
    volume.detach(attachmentID=attachment_id_1)

    # Step 10
    volume = common.wait_for_volume_migration_node(client,
                                                   volume_name,
                                                   local_node)

    # Step 11
    set_node_scheduling(client, schedule_node,
                        allowScheduling=True, retry=True)
    volume = common.wait_for_volume_healthy(client, volume_name)

    # Step 12
    common.check_volume_data(volume, data)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="tests" href="index.html">tests</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="tests.test_migration.check_detached_volume_data" href="#tests.test_migration.check_detached_volume_data">check_detached_volume_data</a></code></li>
<li><code><a title="tests.test_migration.get_hosts_for_migration_test" href="#tests.test_migration.get_hosts_for_migration_test">get_hosts_for_migration_test</a></code></li>
<li><code><a title="tests.test_migration.migration_confirm_test" href="#tests.test_migration.migration_confirm_test">migration_confirm_test</a></code></li>
<li><code><a title="tests.test_migration.migration_rollback_test" href="#tests.test_migration.migration_rollback_test">migration_rollback_test</a></code></li>
<li><code><a title="tests.test_migration.setup_migration_test" href="#tests.test_migration.setup_migration_test">setup_migration_test</a></code></li>
<li><code><a title="tests.test_migration.test_migration_confirm" href="#tests.test_migration.test_migration_confirm">test_migration_confirm</a></code></li>
<li><code><a title="tests.test_migration.test_migration_rollback" href="#tests.test_migration.test_migration_rollback">test_migration_rollback</a></code></li>
<li><code><a title="tests.test_migration.test_migration_with_failed_replica" href="#tests.test_migration.test_migration_with_failed_replica">test_migration_with_failed_replica</a></code></li>
<li><code><a title="tests.test_migration.test_migration_with_rebuilding_replica" href="#tests.test_migration.test_migration_with_rebuilding_replica">test_migration_with_rebuilding_replica</a></code></li>
<li><code><a title="tests.test_migration.test_migration_with_restore_volume" href="#tests.test_migration.test_migration_with_restore_volume">test_migration_with_restore_volume</a></code></li>
<li><code><a title="tests.test_migration.test_migration_with_unscheduled_replica" href="#tests.test_migration.test_migration_with_unscheduled_replica">test_migration_with_unscheduled_replica</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>